{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24c7a759-c797-4844-a14c-b242c32028c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audit table updated for BCH: LastLoaded set to 2024-11-05 20:28:00\n",
      "Audit table updated for BTC: LastLoaded set to 2024-11-05 20:28:00\n",
      "Audit table updated for ETH: LastLoaded set to 2024-11-05 20:28:00\n",
      "Audit table updated for SOL: LastLoaded set to 2024-11-05 20:28:00\n",
      "Audit table updated for XRP: LastLoaded set to 2024-11-05 20:28:00\n",
      "Data saved for BCH from 2024\n",
      "Data saved for XRP from 2024\n",
      "Data saved for ETH from 2024\n",
      "Data saved for BTC from 2024\n",
      "Data saved for SOL from 2024\n",
      "Execution time: 17.55 seconds\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import concurrent.futures\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Apply nest_asyncio for Jupyter Notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Constants and API endpoint\n",
    "API_URL = \"https://api.binance.com/api/v3/klines\"\n",
    "CURRENCIES = [\"XRP\", \"SOL\", \"ETH\", \"BTC\", \"BCH\"]  # Changed to base currency only\n",
    "YEARLY_FILE_2024 = \"E:/Data/crypto_2024.csv\"  # Only 2024 file for incremental loads\n",
    "AUDIT_FILE = \"E:/Data/audit_table.csv\"\n",
    "\n",
    "# Function to load the audit table\n",
    "def load_audit_table():\n",
    "    if os.path.exists(AUDIT_FILE):\n",
    "        audit_df = pd.read_csv(AUDIT_FILE)\n",
    "    else:\n",
    "        audit_df = pd.DataFrame(columns=[\"Symbol\", \"LastLoaded\"])\n",
    "    return audit_df\n",
    "\n",
    "# Function to update the audit table\n",
    "def update_audit_table(symbol, last_loaded):\n",
    "    audit_df = load_audit_table()\n",
    "    audit_df = audit_df[audit_df['Symbol'] != symbol]  # Remove old entry for this symbol\n",
    "    new_entry = pd.DataFrame({\"Symbol\": [symbol], \"LastLoaded\": [last_loaded]})\n",
    "    audit_df = pd.concat([audit_df, new_entry], ignore_index=True)\n",
    "    audit_df.to_csv(AUDIT_FILE, index=False)\n",
    "    print(f\"Audit table updated for {symbol}: LastLoaded set to {last_loaded}\")\n",
    "\n",
    "# Function to fetch historical data\n",
    "async def fetch_data(session, symbol, start, end):\n",
    "    params = {\"symbol\": f\"{symbol}USDT\", \"interval\": \"1m\", \"startTime\": start, \"endTime\": end, \"limit\": 1000}\n",
    "    retries = 5\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with session.get(API_URL, params=params) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    return [\n",
    "                        {\"Timestamp\": datetime.fromtimestamp(item[0] / 1000), \"Price\": float(item[4]), \"Symbol\": symbol}\n",
    "                        for item in data\n",
    "                    ]\n",
    "                else:\n",
    "                    print(f\"Error {response.status} for {symbol}, attempt {attempt + 1}\")\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error fetching data for {symbol}, attempt {attempt + 1}: {e}\")\n",
    "        await asyncio.sleep(2 ** attempt)  # Exponential backoff\n",
    "    return []\n",
    "\n",
    "# Function to append data to the 2024 file\n",
    "def append_to_csv(data):\n",
    "    file_exists = os.path.exists(YEARLY_FILE_2024)\n",
    "    with open(YEARLY_FILE_2024, mode=\"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"Timestamp\", \"Price\", \"Symbol\"])\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Function to fetch and save data incrementally based on the audit table\n",
    "async def fetch_and_save_data(symbol, start_date, end_date):\n",
    "    data = []\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        current = start_date\n",
    "        while current < end_date:\n",
    "            end = min(current + timedelta(minutes=1000), end_date)\n",
    "            batch = await fetch_data(session, symbol, int(current.timestamp() * 1000), int(end.timestamp() * 1000))\n",
    "            if batch:\n",
    "                data.extend(batch)\n",
    "                current = batch[-1][\"Timestamp\"] + timedelta(minutes=1)\n",
    "                update_audit_table(symbol, current.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            else:\n",
    "                current = end  # Move forward even if no data returned\n",
    "    append_to_csv(data)\n",
    "    print(f\"Data saved for {symbol} from {start_date.year}\")\n",
    "\n",
    "# Main function with concurrency\n",
    "async def main():\n",
    "    audit_df = load_audit_table()\n",
    "    end_date_2024 = datetime(2024, 12, 31, 23, 59)  # Adjust this to the current date\n",
    "\n",
    "    tasks = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Using ThreadPoolExecutor to parallelize fetching for each symbol\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        for symbol in CURRENCIES:\n",
    "            last_loaded_str = audit_df[audit_df[\"Symbol\"] == symbol][\"LastLoaded\"].max()\n",
    "            last_loaded = datetime.strptime(last_loaded_str, \"%Y-%m-%d %H:%M:%S\") if pd.notna(last_loaded_str) else None\n",
    "\n",
    "            # Start from last loaded timestamp if available, else from start of 2024\n",
    "            start_date = last_loaded + timedelta(minutes=1) if last_loaded else datetime(2024, 1, 1)\n",
    "            tasks.append(fetch_and_save_data(symbol, start_date, end_date_2024))\n",
    "\n",
    "        # Execute all tasks concurrently\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Run the main function with asyncio\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71aa19a-a225-479f-85dc-a16ba44a72dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyspark-env)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
